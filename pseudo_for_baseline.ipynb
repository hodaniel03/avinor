{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "bc265d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importere pakker\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6af8fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/Users/hodanielkhuu/Downloads/202509_Datasett\")\n",
    "KEY = [\"airport_group\", \"date\", \"hour\"]\n",
    "GROUP, DATE, HOUR, DOW = KEY[0], KEY[1], KEY[2], \"dow\"\n",
    "\n",
    "df_train = pd.read_csv(base / \"training_data.csv\", parse_dates=[DATE])\n",
    "df_infer = pd.read_csv(base / \"inference_data_oct2025.csv\", parse_dates=[DATE])\n",
    "df_mal = pd.read_csv(base / \"preds_mal.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "355db266",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mal[DATE] = pd.to_datetime(df_mal[DATE])\n",
    "\n",
    "for name, df in ((\"train\", df_train), (\"mal\", df_mal)):\n",
    "    dupes = df.duplicated(KEY).sum()\n",
    "    assert dupes == 0, f\"{name} has {dupes} duplicate keys\"\n",
    "\n",
    "infer_keys = set(map(tuple, df_infer[KEY].to_numpy()))\n",
    "mal_keys = set(map(tuple, df_mal[KEY].to_numpy()))\n",
    "assert infer_keys == mal_keys, \"Mismatch between inference and mal keys\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "eb013d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[DOW] = df_train[DATE].dt.dayofweek\n",
    "df_mal[DOW]   = df_mal[DATE].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "a0b71750",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_month = df_train[DATE].dt.to_period(\"M\").max()\n",
    "mask_val = df_train[DATE].dt.to_period(\"M\") == last_month\n",
    "train_hist = df_train.loc[~mask_val]\n",
    "valid = df_train.loc[mask_val]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "77a08565",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_hist.empty:\n",
    "    raise ValueError(\"Training history is empty; need at least two months.\")\n",
    "\n",
    "rate_global = train_hist[\"target\"].mean()\n",
    "alpha = 20.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a3d750e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rate_table(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    agg = (\n",
    "        df.groupby(cols)\n",
    "        .agg(count=(\"target\", \"size\"), sum1=(\"target\", \"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg[\"rate\"] = agg[\"sum1\"] / agg[\"count\"]\n",
    "    agg[\"rate_smoothed\"] = (agg[\"sum1\"] + alpha * rate_global) / (agg[\"count\"] + alpha)\n",
    "    return agg\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "c7e8e5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl_A = build_rate_table(train_hist, [GROUP, HOUR])\n",
    "tbl_B = build_rate_table(train_hist, [GROUP, DOW, HOUR])\n",
    "\n",
    "tbl_A_idx = tbl_A.set_index([GROUP, HOUR])\n",
    "tbl_B_idx = tbl_B.set_index([GROUP, DOW, HOUR])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2cc56d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_A(df_feat: pd.DataFrame) -> pd.Series:\n",
    "    merged = df_feat.join(tbl_A_idx, on = [GROUP, HOUR])\n",
    "    preds = merged[\"rate_smoothed\"].combine_first(merged[\"rate\"])\n",
    "    return preds.fillna(rate_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1adfda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_fallback(df_feat: pd.DataFrame) -> tuple[pd.Series, pd.Series]:\n",
    "    merged = df_feat.join(tbl_B_idx, on =[GROUP, DOW, HOUR])\n",
    "    preds = merged[\"rate_smoothed\"].combine_first(merged[\"rate\"])\n",
    "    source = pd.Series(\"B\", index= df_feat.index, dtype=object)\n",
    "\n",
    "    missing = preds.isna()\n",
    "    if missing.any():\n",
    "        fallback = df_feat.loc[missing].join(tbl_A_idx, on= [GROUP, HOUR])\n",
    "        preds.loc[missing] = fallback[\"rate_smoothed\"].combine_first(fallback[\"rate\"])\n",
    "        source.loc[missing] = \"A\"\n",
    "\n",
    "    missing = preds.isna()\n",
    "    if missing.any(): \n",
    "        preds.loc[missing] = rate_global\n",
    "        source.loc[missing] = \"GLOBAL\"\n",
    "\n",
    "    return preds.astype(float), source "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "6cc164ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X = valid[[GROUP, DATE, HOUR, DOW]].copy()\n",
    "valid_y = valid[\"target\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "479c11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pA_valid = predict_A(valid_X).clip(0.0, 1.0)\n",
    "pB_valid, _ = predict_with_fallback(valid_X)\n",
    "pB_valid = pB_valid.clip(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5d1bf948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC A: 0.7878585627007095\n",
      "Brier A: 0.1537974626394816\n",
      "AUC B: 0.8325313305187022\n",
      "Brier B: 0.1413057350677002\n"
     ]
    }
   ],
   "source": [
    "print(\"AUC A:\", roc_auc_score(valid_y, pA_valid))\n",
    "print(\"Brier A:\", brier_score_loss(valid_y, pA_valid))\n",
    "print(\"AUC B:\", roc_auc_score(valid_y, pB_valid))\n",
    "print(\"Brier B:\", brier_score_loss(valid_y, pB_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1d29a1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  group    n     auc_A     auc_B\n",
      "0     A  721  0.669597  0.715590\n",
      "1     B  721  0.797966  0.868869\n",
      "2     C  721  0.691979  0.786030\n",
      "3     D  721  0.661273  0.777762\n",
      "4     E  721  0.850581  0.871968\n"
     ]
    }
   ],
   "source": [
    "group_rows = []\n",
    "for g, df_g in valid.groupby(GROUP):\n",
    "    if df_g[\"target\"].nunique() < 2:\n",
    "        group_rows.append((g, len(df_g), np.nan, np.nan))\n",
    "        continue\n",
    "    idx = df_g.index\n",
    "    group_rows.append(\n",
    "        (\n",
    "            g,\n",
    "            len(df_g),\n",
    "            roc_auc_score(valid_y.loc[idx], pA_valid.loc[idx]),\n",
    "            roc_auc_score(valid_y.loc[idx], pB_valid.loc[idx]),\n",
    "        )\n",
    "    )\n",
    "group_metrics = pd.DataFrame(group_rows, columns=[\"group\", \"n\", \"auc_A\", \"auc_B\"])\n",
    "print(group_metrics.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eae17a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallback usage: {'B': 1.0}\n"
     ]
    }
   ],
   "source": [
    "infer_X = df_infer[[GROUP, DATE, HOUR, DOW]].copy()\n",
    "p_infer, source_tags = predict_with_fallback(infer_X)\n",
    "p_infer = p_infer.clip(0.0, 1.0).round(3)\n",
    "assert p_infer.between(0.0, 1.0).all()\n",
    "\n",
    "fallback_share = source_tags.value_counts(normalize=True).sort_index()\n",
    "print(\"Fallback usage:\", fallback_share.to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "38414928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs/preds_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "pred_df = df_infer[KEY].copy()\n",
    "pred_df[\"pred\"] = p_infer.values\n",
    "\n",
    "submission = df_mal.merge(pred_df, on=KEY, how=\"left\", sort=False)\n",
    "# After merge, 'pred' from df_mal becomes 'pred_x', and from pred_df becomes 'pred_y'\n",
    "# Select the new predictions and rename to 'pred'\n",
    "submission = submission[KEY + [\"pred_y\"]].rename(columns={\"pred_y\": \"pred\"})\n",
    "assert submission[\"pred\"].notna().all()\n",
    "assert submission[\"pred\"].between(0.0, 1.0).all()\n",
    "assert list(submission.columns) == KEY + [\"pred\"]\n",
    "\n",
    "Path(\"outputs\").mkdir(exist_ok=True)\n",
    "submission.to_csv(Path(\"/Users/hodanielkhuu/vscode/avinor/outputs\") / \"preds_baseline.csv\", index=False)\n",
    "print(\"Saved outputs/preds_baseline.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b765577",
   "metadata": {},
   "source": [
    "### PSEUDO CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5bc082",
   "metadata": {},
   "source": [
    "# Konstanter og nøkler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "160e8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = [\"airport_group\", \"date\", \"hour\"]\n",
    "GROUP = \"airport_group\"\n",
    "DATE = \"date\"\n",
    "HOUR = \"hour\"\n",
    "DOW  = \"dow\"   # 0=Mon..6=Sun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d8b67",
   "metadata": {},
   "source": [
    "# Laste data og grunnsjekk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77cfbcf1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[109], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_train  \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m read_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m df_infer  \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m read_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minference_data_oct2025.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m df_mal    \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m read_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpreds_mal.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_csv' is not defined"
     ]
    }
   ],
   "source": [
    "df_train  <- read_csv(\"training_data.csv\")\n",
    "df_infer  <- read_csv(\"inference_data_oct2025.csv\")\n",
    "df_mal    <- read_csv(\"preds_mal.csv\")\n",
    "\n",
    "assert columns_exist(df_train,  KEY + [\"target\"])\n",
    "assert columns_exist(df_infer,  KEY)\n",
    "assert columns_exist(df_mal,    KEY + [\"pred\"])\n",
    "\n",
    "assert unique_keys(df_train, KEY)     # no dupes\n",
    "assert unique_keys(df_mal,   KEY)     # no dupes\n",
    "\n",
    "# parse date\n",
    "df_train[DATE] <- to_datetime(df_train[DATE])\n",
    "df_infer[DATE] <- to_datetime(df_infer[DATE])\n",
    "\n",
    "# derive weekday\n",
    "df_train[DOW] <- weekday(df_train[DATE])   # 0..6\n",
    "df_infer[DOW] <- weekday(df_infer[DATE])\n",
    "\n",
    "# quick shapes to log\n",
    "log_shape(df_train, df_infer, df_mal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c608b559",
   "metadata": {},
   "source": [
    "# Definere tids-splitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5ff6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_month_in_train <- max_month(df_train[DATE])       # e.g. \"2025-09\"\n",
    "mask_val  <- month(df_train[DATE]) == last_month_in_train\n",
    "mask_hist <- month(df_train[DATE])  < last_month_in_train\n",
    "\n",
    "train_hist <- df_train[mask_hist]   # for å beregne rater\n",
    "valid_mon  <- df_train[mask_val]    # siste måned – for evaluering\n",
    "\n",
    "assert not_empty(train_hist)\n",
    "assert not_empty(valid_mon)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11880b9",
   "metadata": {},
   "source": [
    "# Beregn rater kun fra train-hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2adf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_global <- mean(train_hist[\"target\"])\n",
    "\n",
    "tbl_A <- groupby(train_hist, [GROUP, HOUR]) \n",
    "         |> agg(\n",
    "             count = n(),\n",
    "             sum1  = sum(target),\n",
    "             rate  = sum1 / count\n",
    "         )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b717f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valgfri justering mot global rate\n",
    "alpha = 20   # justert etter datastørrelse\n",
    "tbl_A[\"rate_smoothed\"] = (sum1 + alpha*rate_global) / (count + alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c653086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lage baseline for B: rate per GROUP, DOW, HOUR\n",
    "tbl_B <- groupby(train_hist, [GROUP, DOW, HOUR])\n",
    "         |> agg(\n",
    "             count = n(),\n",
    "             sum1  = sum(target),\n",
    "             rate  = sum1 / count\n",
    "         )\n",
    "\n",
    "tbl_B[\"rate_smoothed\"] = (sum1 + alpha*rate_global) / (count + alpha)  # valgfri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff4b61",
   "metadata": {},
   "source": [
    "# Lage predikasjoner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9758b",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_X <- select(valid_mon, [GROUP, DATE, HOUR, DOW])\n",
    "valid_y <- valid_mon[\"target\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9196d04",
   "metadata": {},
   "source": [
    "# Slå opp predikasjoenr: A baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_A <- left_join(valid_X, tbl_A, on=[GROUP,HOUR])\n",
    "pA     <- if_not_null(pred_A[\"rate_smoothed\"], pred_A[\"rate\"], fallback=NULL)\n",
    "\n",
    "# Fallbacks:\n",
    "# if pA is NULL → try rate_global\n",
    "pA <- fillna(pA, rate_global)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a009e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lage predikasjoenr for baseline B\n",
    "pred_B <- left_join(valid_X, tbl_B, on=[GROUP,DOW,HOUR])\n",
    "pB     <- choose(\n",
    "           if_not_null(pred_B[\"rate_smoothed\"], pred_B[\"rate\"]),\n",
    "           # fallback 1: bruk (g,h)\n",
    "           lookup(tbl_A, [GROUP,HOUR], valid_X) -> rate_smoothed or rate,\n",
    "           # fallback 2: global\n",
    "           rate_global\n",
    "         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4aae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# klipp og logg\n",
    "pA <- clip(pA, 0, 1)\n",
    "pB <- clip(pB, 0, 1)\n",
    "\n",
    "log_summary_stats(pA, pB)  # min/max/mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b1588f",
   "metadata": {},
   "source": [
    "# Evaluere baseline på valideringsmåned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_A   <- roc_auc(valid_y, pA)\n",
    "brier_A <- brier_score(valid_y, pA)\n",
    "\n",
    "auc_B   <- roc_auc(valid_y, pB)\n",
    "brier_B <- brier_score(valid_y, pB)\n",
    "\n",
    "# Per-gruppe AUC – viktig for fairness/robusthet\n",
    "by_group_metrics <- for each g in unique(valid_X[GROUP]):\n",
    "    idx = (valid_X[GROUP] == g)\n",
    "    return {\n",
    "      group: g,\n",
    "      n: sum(idx),\n",
    "      auc_A_g: roc_auc(valid_y[idx], pA[idx]),\n",
    "      auc_B_g: roc_auc(valid_y[idx], pB[idx]),\n",
    "      brier_A_g: brier_score(valid_y[idx], pA[idx]),\n",
    "      brier_B_g: brier_score(valid_y[idx], pB[idx]),\n",
    "    }\n",
    "\n",
    "print_table(auc_A, brier_A, auc_B, brier_B)\n",
    "print_table(by_group_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e265c9",
   "metadata": {},
   "source": [
    "# Bruke baseline til inference (fremtid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9a9c2049",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'infer_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m infer_X \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m-\u001b[39m select(df_infer, [GROUP, DATE, HOUR, DOW])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'infer_X' is not defined"
     ]
    }
   ],
   "source": [
    "infer_X <- select(df_infer, [GROUP, DATE, HOUR, DOW])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809c1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_infer <- choose(\n",
    "  lookup(tbl_B, [GROUP,DOW,HOUR]) -> rate_smoothed or rate,\n",
    "  lookup(tbl_A, [GROUP,HOUR])     -> rate_smoothed or rate,\n",
    "  rate_global\n",
    ")\n",
    "\n",
    "p_infer <- clip(p_infer, 0, 1)\n",
    "p_infer <- round(p_infer, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub <- left_join(df_mal[[GROUP,DATE,HOUR]], \n",
    "                 data_frame([GROUP,DATE,HOUR,p_infer]),\n",
    "                 on=KEY)\n",
    "\n",
    "rename sub[p_infer] -> sub[\"pred\"]\n",
    "\n",
    "assert not_null(sub[\"pred\"])\n",
    "assert all_between(sub[\"pred\"], 0, 1)\n",
    "assert columns_equal(sub.columns, [\"airport_group\",\"date\",\"hour\",\"pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output submission\n",
    "write_csv(sub, \"preds_baseline.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
