{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "163445ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e24e37",
   "metadata": {},
   "source": [
    "### Sette basen for de ulike dataene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "00a5605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/Users/hodanielkhuu/Downloads/202509_Datasett\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d8bd5c",
   "metadata": {},
   "source": [
    "### Importere de nødvendige filene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "91e525ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(base / \"training_data.csv\")\n",
    "df_infer = pd.read_csv(base / \"inference_data_oct2025.csv\")\n",
    "df_mal = pd.read_csv(base / \"preds_mal.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "eff4abfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (465031, 7)\n",
      "Infer Shape: (5208, 6)\n",
      "Mal Shape: (5208, 4)\n",
      "\n",
      "Train Columns: ['airport_group', 'date', 'hour', 'target', 'feat_season', 'feat_sched_flights_cnt', 'feat_sched_concurrence']\n",
      "Infer Columns: ['airport_group', 'date', 'hour', 'feat_season', 'feat_sched_flights_cnt', 'feat_sched_concurrence']\n",
      "Mal Columns: ['airport_group', 'date', 'hour', 'pred']\n"
     ]
    }
   ],
   "source": [
    "# Viser kolonnenavn og antall rader\n",
    "print(\"Train Shape:\", df_train.shape)\n",
    "print(\"Infer Shape:\", df_infer.shape)\n",
    "print(\"Mal Shape:\", df_mal.shape)\n",
    "\n",
    "print(\"\\nTrain Columns:\", df_train.columns.tolist())\n",
    "print(\"Infer Columns:\", df_infer.columns.tolist())\n",
    "print(\"Mal Columns:\", df_mal.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "71f26c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingen duplikater funnet i treningsdata.\n"
     ]
    }
   ],
   "source": [
    "## Finne duplikater rader i treningsdata, trenger bare for disse kolonnene fordi det er det som sørger for at belastningen er riktig.\n",
    "key_cols = ['airport_group', 'date', 'hour']\n",
    "n_rows = len(df_train)\n",
    "n_unique_rows = df_train[key_cols].drop_duplicates().shape[0]\n",
    "\n",
    "if n_rows == n_unique_rows:\n",
    "    print(\"Ingen duplikater funnet i treningsdata.\")\n",
    "else:\n",
    "    print(f\"Funnet {n_rows - n_unique_rows} duplikater i treningsdata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "65815e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kolonner i train, men ikke i infer: {'target'}\n",
      "\n",
      "Kolonner i infer, men ikke i train: set()\n"
     ]
    }
   ],
   "source": [
    "# Sammenlinge train og infer data for å se hva vi skal predikere\n",
    "cols_train = set(df_train.columns)\n",
    "cols_infer = set(df_infer.columns)\n",
    "\n",
    "extra_in_train = cols_train - cols_infer\n",
    "extra_in_infer = cols_infer - cols_train\n",
    "\n",
    "print(\"\\nKolonner i train, men ikke i infer:\", extra_in_train)\n",
    "print(\"\\nKolonner i infer, men ikke i train:\", extra_in_infer)\n",
    "# Blir riktig å finne target fordi vi skal predikere dette."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a52b0a8",
   "metadata": {},
   "source": [
    "## Kopiere baseline over hit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "b4da6e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importere pakker\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "df1fb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Path(\"/Users/hodanielkhuu/Downloads/202509_Datasett\")\n",
    "KEY = [\"airport_group\", \"date\", \"hour\"]\n",
    "GROUP, DATE, HOUR, DOW = KEY[0], KEY[1], KEY[2], \"dow\"\n",
    "\n",
    "df_train = pd.read_csv(base / \"training_data.csv\", parse_dates=[DATE])\n",
    "df_infer = pd.read_csv(base / \"inference_data_oct2025.csv\", parse_dates=[DATE])\n",
    "df_mal = pd.read_csv(base / \"preds_mal.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec3d54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "aa0ccdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (df_train, df_infer, df_mal):\n",
    "    df[DATE] = pd.to_datetime(df[DATE])\n",
    "    df[HOUR] = df[HOUR].astype(int)\n",
    "\n",
    "\n",
    "for name, df in ((\"train\", df_train), (\"mal\", df_mal)):\n",
    "    dupes = df.duplicated(KEY).sum()\n",
    "    assert dupes == 0, f\"{name} has {dupes} duplicate keys\"\n",
    "\n",
    "infer_keys = set(map(tuple, df_infer[KEY].to_numpy()))\n",
    "mal_keys = set(map(tuple, df_mal[KEY].to_numpy()))\n",
    "assert infer_keys == mal_keys, \"Mismatch between inference and mal keys\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2608212",
   "metadata": {},
   "source": [
    "## Calenders features and time split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "41fc8ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: 2018-01-01 00:00:00 → 2025-06-30 00:00:00\n",
      "Valid range: 2025-07-01 00:00:00 → 2025-07-31 00:00:00\n",
      "Last month: 2025-07\n"
     ]
    }
   ],
   "source": [
    "for df in (df_train, df_infer, df_mal):\n",
    "    df[DOW] = df[DATE].dt.dayofweek\n",
    "\n",
    "last_month = df_train[DATE].dt.to_period(\"M\").max()\n",
    "mask_val = df_train[DATE].dt.to_period(\"M\") == last_month\n",
    "train_hist = df_train.loc[~mask_val].copy()\n",
    "valid = df_train.loc[mask_val].copy()\n",
    "\n",
    "if train_hist.empty:\n",
    "    raise ValueError(\"Training history is empty; need at least two months.\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Train range:\", train_hist[DATE].min(), \"→\", train_hist[DATE].max())\n",
    "print(\"Valid range:\",  valid[DATE].min(),      \"→\", valid[DATE].max())\n",
    "print(\"Last month:\", last_month)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15f053",
   "metadata": {},
   "source": [
    "## Historical Rates Tables from train_hist only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2082fbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate tables built. Sizes: 168 1176 rate_global= 0.2219\n"
     ]
    }
   ],
   "source": [
    "rate_global = train_hist[\"target\"].mean()\n",
    "alpha = 20.0\n",
    "\n",
    "\n",
    "def build_rate_table(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:\n",
    "    assert \"target\" in df.columns and len(df) > 0\n",
    "    agg = (\n",
    "        df.groupby(cols, dropna=False)\n",
    "        .agg(count=(\"target\", \"size\"), sum1=(\"target\", \"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    agg[\"rate\"] = np.where(agg[\"count\"] > 0, agg[\"sum1\"] / agg[\"count\"], rate_global)\n",
    "    agg[\"rate_smoothed\"] = (agg[\"sum1\"] + alpha * rate_global) / (agg[\"count\"] + alpha)\n",
    "    return agg\n",
    "\n",
    "tbl_A = build_rate_table(train_hist, [GROUP, HOUR])\n",
    "tbl_B = build_rate_table(train_hist, [GROUP, DOW, HOUR])\n",
    "\n",
    "# Keep only smoothed rates and rename to feature names\n",
    "tbl_A_feat = tbl_A[[GROUP, HOUR, \"rate_smoothed\"]].rename(\n",
    "    columns={\"rate_smoothed\":\"rate_group_hour\"}\n",
    ")\n",
    "tbl_B_feat = tbl_B[[GROUP, DOW, HOUR, \"rate_smoothed\"]].rename(\n",
    "    columns={\"rate_smoothed\":\"rate_group_dow_hour\"}\n",
    ")\n",
    "print(\"Rate tables built. Sizes:\", len(tbl_A_feat), len(tbl_B_feat), \"rate_global=\", round(rate_global,4))\n",
    "\n",
    "\n",
    "valid_X = valid[[GROUP, DATE, HOUR, DOW]].copy()\n",
    "valid_y = valid[\"target\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99192a7",
   "metadata": {},
   "source": [
    "### Join Rate features and simple calender flags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "7683b83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate features joined and calendar flags added.\n"
     ]
    }
   ],
   "source": [
    "def join_rates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.merge(tbl_A_feat, on=[GROUP, HOUR], how =\"left\")\n",
    "    out = out.merge(tbl_B_feat, on=[GROUP, DOW, HOUR], how=\"left\")\n",
    "    out[\"rate_group_hour\"] = out[\"rate_group_hour\"].fillna(rate_global)\n",
    "    out[\"rate_group_dow_hour\"] = out[\"rate_group_dow_hour\"].fillna(rate_global)\n",
    "    return out\n",
    "\n",
    "train_hist = join_rates(train_hist)\n",
    "valid = join_rates(valid)\n",
    "df_infer = join_rates(df_infer)\n",
    "\n",
    "for df in (train_hist,valid,df_infer):\n",
    "    df[\"is_weekend\"] = (df[DOW] >= 5).astype(int)\n",
    "\n",
    "print(\"Rate features joined and calendar flags added.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbbbf6b",
   "metadata": {},
   "source": [
    "### Lag features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "2aa60f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lag features created.\n"
     ]
    }
   ],
   "source": [
    "# --- Part 6: Lag features (strictly past within group) ---\n",
    "def add_lags(df: pd.DataFrame, group_col: str, cols: list[str], lags=(1,)) -> pd.DataFrame:\n",
    "    out = df.sort_values([group_col, DATE, HOUR]).copy()\n",
    "    for c in cols:\n",
    "        for l in lags:\n",
    "            out[f\"{c}_lag{l}\"] = out.groupby(group_col)[c].shift(l)\n",
    "    return out\n",
    "\n",
    "lag_cols = [\"feat_sched_flights_cnt\", \"feat_sched_concurrence\"]\n",
    "\n",
    "train_hist = add_lags(train_hist, GROUP, lag_cols, lags=(1,))\n",
    "valid      = add_lags(valid,      GROUP, lag_cols, lags=(1,))\n",
    "df_infer   = add_lags(df_infer,   GROUP, lag_cols, lags=(1,))\n",
    "\n",
    "# First hour per group has NaN lags → fill with 0\n",
    "for df in (train_hist, valid, df_infer):\n",
    "    for c in [f\"{x}_lag1\" for x in lag_cols]:\n",
    "        df[c] = df[c].fillna(0.0)\n",
    "\n",
    "print(\"Lag features created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5df710b",
   "metadata": {},
   "source": [
    "# --- Part 7: Freeze feature list & build X/y ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "749e6d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shapes: (459984, 9) (5047, 9)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = [\n",
    "    \"hour\",\n",
    "    DOW,\n",
    "    \"is_weekend\",\n",
    "    \"feat_sched_flights_cnt\",\n",
    "    \"feat_sched_concurrence\",\n",
    "    \"feat_sched_flights_cnt_lag1\",\n",
    "    \"feat_sched_concurrence_lag1\",\n",
    "    \"rate_group_hour\",\n",
    "    \"rate_group_dow_hour\",\n",
    "    # Optional cyclics later: \"hour_sin\", \"hour_cos\",\n",
    "]\n",
    "\n",
    "X_train = train_hist[feature_cols].copy()\n",
    "y_train = train_hist[\"target\"].astype(int)\n",
    "\n",
    "X_val = valid[feature_cols].copy()\n",
    "y_val = valid[\"target\"].astype(int)\n",
    "\n",
    "single_class_val = (y_val.nunique() < 2)\n",
    "print(\"Feature matrix shapes:\", X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "4380bd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR]   AUC=0.9640\n",
      "[LR]   Brier=0.0582\n"
     ]
    }
   ],
   "source": [
    "# --- Part 8: Logistic Regression (scaled) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "lr = LogisticRegression(max_iter=500, solver=\"lbfgs\")\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "pred_lr = lr.predict_proba(X_val_scaled)[:,1]\n",
    "\n",
    "auc_lr   = np.nan if single_class_val else roc_auc_score(y_val, pred_lr)\n",
    "brier_lr = brier_score_loss(y_val, pred_lr)\n",
    "\n",
    "print(f\"[LR]   AUC={auc_lr:.4f}\" if not np.isnan(auc_lr) else \"[LR]   AUC=NA (single-class val)\")\n",
    "print(f\"[LR]   Brier={brier_lr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22735bdd",
   "metadata": {},
   "source": [
    "## HistgradientBoosting classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "47dcc862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HGB]  AUC=0.9731\n",
      "[HGB] Brier=0.0503\n"
     ]
    }
   ],
   "source": [
    "hgb =  HistGradientBoostingClassifier(\n",
    "    learning_rate = 0.05,\n",
    "    max_iter=400,\n",
    "    min_samples_leaf = 40,\n",
    "    random_state = 42\n",
    ")\n",
    "\n",
    "hgb.fit(X_train, y_train)\n",
    "pred_hgb = hgb.predict_proba(X_val)[:,1]\n",
    "\n",
    "auc_hgb = np.nan if single_class_val else roc_auc_score(y_val, pred_hgb)\n",
    "brier_hgb = brier_score_loss(y_val, pred_hgb)\n",
    "\n",
    "print(f\"[HGB]  AUC={auc_hgb:.4f}\" if not np.isnan(auc_hgb) else \"[HGB]  AUC=NA (single-class val)\")\n",
    "print(f\"[HGB] Brier={brier_hgb:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ed2709",
   "metadata": {},
   "source": [
    "## prefer AUC else Brier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c83f5464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected best model: HGB\n"
     ]
    }
   ],
   "source": [
    "def pick_best(auc_lr, brier_lr, auc_hgb, brier_hgb):\n",
    "    if not np.isnan(auc_lr) and not np.isnan(auc_hgb):\n",
    "        return (\"hgb\", hgb) if auc_hgb >= auc_lr else (\"lr\", lr)\n",
    "\n",
    "    return (\"hgb\", hgb) if brier_hgb <= brier_lr else (\"lr\", lr)\n",
    "\n",
    "best_name, best_model = pick_best(auc_lr, brier_lr, auc_hgb, brier_hgb)\n",
    "print(f\"Selected best model: {best_name.upper()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7f197ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved /Users/hodanielkhuu/vscode/avinor/outputs/preds_ml.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Part 11: Inference build → predict → save submission ---\n",
    "X_infer = df_infer[feature_cols].copy()\n",
    "\n",
    "if best_name == \"lr\":\n",
    "    X_infer_scaled = scaler.transform(X_infer)\n",
    "    p_infer = best_model.predict_proba(X_infer_scaled)[:,1]\n",
    "else:\n",
    "    p_infer = best_model.predict_proba(X_infer)[:,1]\n",
    "\n",
    "p_infer = np.clip(p_infer, 0, 1)\n",
    "p_infer = np.round(p_infer, 3)\n",
    "\n",
    "# Build submission dataframe from df_infer keys and the predictions.\n",
    "# This ensures `submission` is defined and has the correct row order.\n",
    "submission = df_infer[KEY].copy()\n",
    "submission[\"pred\"] = p_infer\n",
    "\n",
    "# Basic sanity checks\n",
    "assert submission[\"pred\"].notna().all()\n",
    "assert submission[\"pred\"].between(0, 1).all()\n",
    "\n",
    "# Keep only the competition schema (order matters)\n",
    "submission = submission[KEY + [\"pred\"]]\n",
    "\n",
    "# Final assert to be safe\n",
    "assert list(submission.columns) == KEY + [\"pred\"]\n",
    "\n",
    "# Save to disk. Use save_dir (exists in notebook) and Path from earlier imports.\n",
    "out_path = Path(save_dir) / \"/Users/hodanielkhuu/vscode/avinor/outputs/preds_ml.csv\"\n",
    "submission.to_csv(out_path, index=False)\n",
    "print(f\"Saved {out_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "51731f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_hgb_series = pd.Series(pred_hgb, index=y_val.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "2253b14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  airport_group    n       AUC     Brier\n",
      "0             A  721  0.940967  0.050347\n",
      "1             B  721  0.974890  0.058303\n",
      "2             C  721  0.994713  0.014529\n",
      "3             D  721  0.979583  0.048810\n",
      "4             E  721  0.970005  0.077572\n",
      "5             F  721  0.973726  0.029333\n",
      "6             G  721  0.942403  0.073485\n"
     ]
    }
   ],
   "source": [
    "group_metrics = []\n",
    "for g, df_g in valid.groupby(\"airport_group\"):\n",
    "    if df_g[\"target\"].nunique() < 2:\n",
    "        group_metrics.append((g, len(df_g), np.nan, np.nan))\n",
    "        continue\n",
    "    \n",
    "    idx = df_g.index\n",
    "    \n",
    "    auc = roc_auc_score(y_val.loc[idx], pred_hgb_series.loc[idx])\n",
    "    brier = brier_score_loss(y_val.loc[idx], pred_hgb_series.loc[idx])\n",
    "    \n",
    "    group_metrics.append((g, len(df_g), auc, brier))\n",
    "\n",
    "group_df = pd.DataFrame(group_metrics, columns=[\"airport_group\",\"n\",\"AUC\",\"Brier\"])\n",
    "print(group_df.sort_values(\"n\", ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5653d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da6f5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
